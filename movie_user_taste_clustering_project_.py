# -*- coding: utf-8 -*-
"""Movie User Taste Clustering Project .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14Cafy2LcHeQobI8Z86Pc2dmiCvuYdiSx
"""

!pip install pandas numpy matplotlib seaborn scikit-learn plotly



# Step 1: Import libraries
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

sns.set(style="whitegrid")

# Step 2: Generate synthetic user-movie rating data
np.random.seed(42)
n_users = 100
n_movies = 20

# Simulate 3 user taste profiles
profile_1 = np.random.normal(loc=4, scale=0.5, size=n_movies)  # likes most movies
profile_2 = np.random.normal(loc=2, scale=0.5, size=n_movies)  # dislikes most
profile_3 = np.random.normal(loc=3, scale=0.5, size=n_movies)  # neutral

profiles = [profile_1, profile_2, profile_3]

# Assign users randomly to profile indices
profile_indices = np.random.choice(len(profiles), size=n_users)

# Map indices to actual profiles, adding noise for each user
ratings = np.array([
    profiles[idx] + np.random.normal(0, 0.5, n_movies)
    for idx in profile_indices
])

# Clip ratings between 1 and 5
ratings = np.clip(ratings, 1, 5)

# Create DataFrame
movie_titles = [f"Movie_{i+1}" for i in range(n_movies)]
df = pd.DataFrame(ratings, columns=movie_titles)
df['UserID'] = [f"User_{i+1}" for i in range(n_users)]
df = df.set_index('UserID')

print("Sample user ratings:")
print(df.head())

# Step 3: Preprocess - Standardize data
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df)

# Step 4: Apply PCA to reduce dimensionality to 2D for visualization
pca = PCA(n_components=2)
principal_components = pca.fit_transform(df_scaled)

print(f"\nExplained variance ratio by 2 components: {pca.explained_variance_ratio_}")

# Step 5: Use Elbow Method to find optimal K for KMeans
wcss = []
K_range = range(1, 10)
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(principal_components)
    wcss.append(kmeans.inertia_)

plt.figure(figsize=(8, 4))
plt.plot(K_range, wcss, 'bo-', markersize=8)
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Within-Cluster Sum of Squares (WCSS)')
plt.title('Elbow Method for Optimal K')
plt.show()

# Step 6: Fit KMeans with chosen clusters (e.g., 3)
k_optimal = 3
kmeans = KMeans(n_clusters=k_optimal, random_state=42)
clusters = kmeans.fit_predict(principal_components)

# Step 7: Visualize clusters in PCA space
plt.figure(figsize=(8,6))
palette = sns.color_palette("bright", k_optimal)
sns.scatterplot(x=principal_components[:,0], y=principal_components[:,1], hue=clusters, palette=palette, s=80)
plt.title('User Taste Clusters (PCA Projection)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(title='Cluster')
plt.show()

# Step 8: Add cluster labels back to original DataFrame
df['Cluster'] = clusters
print("\nCluster counts:")
print(df['Cluster'].value_counts())

print("\nCluster Centers (mean ratings per movie):")
print(df.groupby('Cluster').mean())

import numpy as np

# Example profiles (can be multidimensional arrays)
profile_1 = np.array([1, 2, 3])
profile_2 = np.array([4, 5, 6])
profile_3 = np.array([7, 8, 9])

profiles = [profile_1, profile_2, profile_3]

n_users = 10  # number of users to assign

# Step 1: randomly select profile indices for each user
profile_indices = np.random.choice(len(profiles), size=n_users)

# Step 2: assign profiles to users based on indices
user_profiles = [profiles[i] for i in profile_indices]

# Optional: convert list of arrays to a 2D numpy array if all profiles have same shape
user_profiles_array = np.array(user_profiles)

print("Profile indices assigned to users:")
print(profile_indices)

print("\nUser profiles assigned:")
print(user_profiles_array)